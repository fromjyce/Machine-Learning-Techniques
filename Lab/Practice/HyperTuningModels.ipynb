{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "c9XEl-aTMzrR"
      },
      "outputs": [],
      "source": [
        "# Importing necessary libraries\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LinearRegression, Ridge, Lasso, ElasticNet, LogisticRegression\n",
        "from sklearn.tree import DecisionTreeClassifier, DecisionTreeRegressor\n",
        "from sklearn.ensemble import RandomForestClassifier, RandomForestRegressor, GradientBoostingClassifier, GradientBoostingRegressor, AdaBoostClassifier, AdaBoostRegressor, ExtraTreesClassifier, ExtraTreesRegressor, VotingClassifier, VotingRegressor, BaggingClassifier, BaggingRegressor, StackingClassifier, StackingRegressor\n",
        "from sklearn.svm import SVC, SVR\n",
        "from sklearn.cluster import KMeans, DBSCAN\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
        "from sklearn.naive_bayes import GaussianNB, MultinomialNB\n",
        "from sklearn.neighbors import KNeighborsClassifier, KNeighborsRegressor\n",
        "\n",
        "# Example data\n",
        "X, y = np.random.randn(100, 5), np.random.randn(100)\n",
        "\n",
        "# Splitting data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Linear Models\n",
        "linear_regression = LinearRegression()\n",
        "ridge_regression = Ridge()\n",
        "lasso_regression = Lasso()\n",
        "elasticnet_regression = ElasticNet()\n",
        "logistic_regression = LogisticRegression()\n",
        "\n",
        "# Tree-based Models\n",
        "decision_tree_classifier = DecisionTreeClassifier()\n",
        "decision_tree_regressor = DecisionTreeRegressor()\n",
        "random_forest_classifier = RandomForestClassifier()\n",
        "random_forest_regressor = RandomForestRegressor()\n",
        "gradient_boosting_classifier = GradientBoostingClassifier()\n",
        "gradient_boosting_regressor = GradientBoostingRegressor()\n",
        "adaboost_classifier = AdaBoostClassifier()\n",
        "adaboost_regressor = AdaBoostRegressor()\n",
        "extra_trees_classifier = ExtraTreesClassifier()\n",
        "extra_trees_regressor = ExtraTreesRegressor()\n",
        "\n",
        "# Support Vector Machines\n",
        "svc = SVC()\n",
        "svr = SVR()\n",
        "\n",
        "# Clustering\n",
        "kmeans = KMeans()\n",
        "dbscan = DBSCAN()\n",
        "\n",
        "# Dimensionality Reduction\n",
        "pca = PCA()\n",
        "lda = LinearDiscriminantAnalysis()\n",
        "\n",
        "# Ensemble Methods\n",
        "voting_classifier = VotingClassifier(estimators=[('dt', decision_tree_classifier), ('rf', random_forest_classifier), ('svc', svc)])\n",
        "voting_regressor = VotingRegressor(estimators=[('dt', decision_tree_regressor), ('rf', random_forest_regressor), ('svr', svr)])\n",
        "bagging_classifier = BaggingClassifier(base_estimator=DecisionTreeClassifier())\n",
        "bagging_regressor = BaggingRegressor(base_estimator=DecisionTreeRegressor())\n",
        "stacking_classifier = StackingClassifier(estimators=[('dt', decision_tree_classifier), ('rf', random_forest_classifier), ('svc', svc)])\n",
        "stacking_regressor = StackingRegressor(estimators=[('dt', decision_tree_regressor), ('rf', random_forest_regressor), ('svr', svr)])\n",
        "\n",
        "# Naive Bayes\n",
        "gaussian_nb = GaussianNB()\n",
        "multinomial_nb = MultinomialNB()\n",
        "\n",
        "# K-Nearest Neighbors\n",
        "knn_classifier = KNeighborsClassifier()\n",
        "knn_regressor = KNeighborsRegressor()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## GridSearchCV"
      ],
      "metadata": {
        "id": "QZEzQho3OJ0F"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import GridSearchCV\n",
        "\n",
        "# Linear Models\n",
        "linear_regression_params = {'fit_intercept': [True, False]}\n",
        "ridge_regression_params = {'alpha': [0.1, 1.0, 10.0]}\n",
        "lasso_regression_params = {'alpha': [0.1, 1.0, 10.0]}\n",
        "elasticnet_regression_params = {'alpha': [0.1, 1.0, 10.0], 'l1_ratio': [0.1, 0.5, 0.9]}\n",
        "logistic_regression_params = {'C': [0.1, 1.0, 10.0]}\n",
        "\n",
        "# Tree-based Models\n",
        "tree_params = {'criterion': ['gini', 'entropy'], 'max_depth': [None, 10, 20]}\n",
        "random_forest_params = {'n_estimators': [50, 100, 200], 'max_depth': [None, 10, 20]}\n",
        "gradient_boosting_params = {'n_estimators': [50, 100, 200], 'learning_rate': [0.1, 0.01, 0.001]}\n",
        "adaboost_params = {'n_estimators': [50, 100, 200], 'learning_rate': [0.1, 0.01, 0.001]}\n",
        "extra_trees_params = {'n_estimators': [50, 100, 200], 'max_depth': [None, 10, 20]}\n",
        "\n",
        "# Support Vector Machines\n",
        "svc_params = {'C': [0.1, 1.0, 10.0], 'kernel': ['linear', 'rbf']}\n",
        "svr_params = {'C': [0.1, 1.0, 10.0], 'kernel': ['linear', 'rbf']}\n",
        "\n",
        "# Clustering\n",
        "kmeans_params = {'n_clusters': [2, 3, 4], 'init': ['k-means++', 'random']}\n",
        "dbscan_params = {'eps': [0.1, 0.5, 1.0], 'min_samples': [5, 10, 20]}\n",
        "\n",
        "# Dimensionality Reduction\n",
        "pca_params = {'n_components': [2, 3, 4]}\n",
        "lda_params = {'n_components': [2]}\n",
        "\n",
        "# Ensemble Methods\n",
        "voting_classifier_params = {'voting': ['hard', 'soft']}\n",
        "voting_regressor_params = {'voting': ['hard', 'soft']}\n",
        "bagging_params = {'n_estimators': [50, 100, 200]}\n",
        "stacking_params = {'final_estimator': [LogisticRegression(max_iter=1000)]}\n",
        "\n",
        "# Naive Bayes (No hyperparameters to tune)\n",
        "\n",
        "# K-Nearest Neighbors\n",
        "knn_classifier_params = {'n_neighbors': [3, 5, 7]}\n",
        "knn_regressor_params = {'n_neighbors': [3, 5, 7]}\n",
        "\n",
        "# Create GridSearchCV instances for each model\n",
        "linear_regression_gs = GridSearchCV(LinearRegression(), linear_regression_params, cv=5)\n",
        "ridge_regression_gs = GridSearchCV(Ridge(), ridge_regression_params, cv=5)\n",
        "lasso_regression_gs = GridSearchCV(Lasso(), lasso_regression_params, cv=5)\n",
        "elasticnet_regression_gs = GridSearchCV(ElasticNet(), elasticnet_regression_params, cv=5)\n",
        "logistic_regression_gs = GridSearchCV(LogisticRegression(), logistic_regression_params, cv=5)\n",
        "\n",
        "tree_gs = GridSearchCV(DecisionTreeClassifier(), tree_params, cv=5)\n",
        "random_forest_gs = GridSearchCV(RandomForestClassifier(), random_forest_params, cv=5)\n",
        "gradient_boosting_gs = GridSearchCV(GradientBoostingClassifier(), gradient_boosting_params, cv=5)\n",
        "adaboost_gs = GridSearchCV(AdaBoostClassifier(), adaboost_params, cv=5)\n",
        "extra_trees_gs = GridSearchCV(ExtraTreesClassifier(), extra_trees_params, cv=5)\n",
        "\n",
        "svc_gs = GridSearchCV(SVC(), svc_params, cv=5)\n",
        "svr_gs = GridSearchCV(SVR(), svr_params, cv=5)\n",
        "\n",
        "kmeans_gs = GridSearchCV(KMeans(), kmeans_params, cv=5)\n",
        "dbscan_gs = GridSearchCV(DBSCAN(), dbscan_params, cv=5)\n",
        "\n",
        "pca_gs = GridSearchCV(PCA(), pca_params, cv=5)\n",
        "lda_gs = GridSearchCV(LinearDiscriminantAnalysis(), lda_params, cv=5)\n",
        "\n",
        "voting_classifier_gs = GridSearchCV(VotingClassifier(estimators=[('dt', decision_tree_classifier), ('rf', random_forest_classifier), ('svc', svc)]), voting_classifier_params, cv=5)\n",
        "voting_regressor_gs = GridSearchCV(VotingRegressor(estimators=[('dt', decision_tree_regressor), ('rf', random_forest_regressor), ('svr', svr)]), voting_regressor_params, cv=5)\n",
        "bagging_gs = GridSearchCV(BaggingClassifier(base_estimator=DecisionTreeClassifier()), bagging_params, cv=5)\n",
        "stacking_gs = GridSearchCV(StackingClassifier(estimators=[('dt', decision_tree_classifier), ('rf', random_forest_classifier), ('svc', svc)]), stacking_params, cv=5)\n",
        "\n",
        "knn_classifier_gs = GridSearchCV(KNeighborsClassifier(), knn_classifier_params, cv=5)\n",
        "knn_regressor_gs = GridSearchCV(KNeighborsRegressor(), knn_regressor_params, cv=5)\n"
      ],
      "metadata": {
        "id": "LC2d2cYGONYw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Randomized Search"
      ],
      "metadata": {
        "id": "dw1VyxJPORxj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "### example\n",
        "\n",
        "from sklearn.model_selection import RandomizedSearchCV\n",
        "random_search = RandomizedSearchCV(estimator=model, param_distributions=param_dist, n_iter=100, cv=5, scoring='accuracy', random_state=42)\n",
        "random_search.fit(X_train_scaled, y_train)\n",
        "print(\"Best parameters:\", random_search.best_params_)\n",
        "print(\"Best cross-validation accuracy:\", random_search.best_score_)"
      ],
      "metadata": {
        "id": "KdttfklkOZ9P"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Importing necessary libraries\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split, RandomizedSearchCV\n",
        "from sklearn.linear_model import LinearRegression, Ridge, Lasso, ElasticNet, LogisticRegression\n",
        "from sklearn.tree import DecisionTreeClassifier, DecisionTreeRegressor\n",
        "from sklearn.ensemble import RandomForestClassifier, RandomForestRegressor, GradientBoostingClassifier, GradientBoostingRegressor, AdaBoostClassifier, AdaBoostRegressor, ExtraTreesClassifier, ExtraTreesRegressor\n",
        "from sklearn.svm import SVC, SVR\n",
        "from sklearn.cluster import KMeans, DBSCAN\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
        "from sklearn.naive_bayes import GaussianNB, MultinomialNB\n",
        "from sklearn.neighbors import KNeighborsClassifier, KNeighborsRegressor\n",
        "\n",
        "# Example data\n",
        "X, y = np.random.randn(100, 5), np.random.randn(100)\n",
        "\n",
        "# Splitting data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Hyperparameter grids for RandomizedSearchCV\n",
        "param_distributions = {\n",
        "    'alpha': [0.01, 0.1, 1.0],\n",
        "    'fit_intercept': [True, False]\n",
        "}\n",
        "\n",
        "tree_param_distributions = {\n",
        "    'max_depth': [None, 10, 20, 30],\n",
        "    'min_samples_split': [2, 5, 10],\n",
        "    'min_samples_leaf': [1, 2, 4],\n",
        "    'max_features': ['auto', 'sqrt', 'log2']\n",
        "}\n",
        "\n",
        "svm_param_distributions = {\n",
        "    'C': [0.1, 1, 10],\n",
        "    'kernel': ['linear', 'poly', 'rbf', 'sigmoid'],\n",
        "    'gamma': ['scale', 'auto']\n",
        "}\n",
        "\n",
        "kmeans_param_distributions = {\n",
        "    'n_clusters': [2, 3, 5, 10],\n",
        "    'init': ['k-means++', 'random'],\n",
        "    'max_iter': [100, 200, 300],\n",
        "    'tol': [1e-3, 1e-4, 1e-5]\n",
        "}\n",
        "\n",
        "# RandomizedSearchCV for each model\n",
        "random_search = RandomizedSearchCV(LinearRegression(), param_distributions=param_distributions, n_iter=10)\n",
        "random_search.fit(X_train, y_train)\n",
        "print(\"Best parameters for Linear Regression:\", random_search.best_params_)\n",
        "\n",
        "random_search = RandomizedSearchCV(Ridge(), param_distributions=param_distributions, n_iter=10)\n",
        "random_search.fit(X_train, y_train)\n",
        "print(\"Best parameters for Ridge Regression:\", random_search.best_params_)\n",
        "\n",
        "random_search = RandomizedSearchCV(Lasso(), param_distributions=param_distributions, n_iter=10)\n",
        "random_search.fit(X_train, y_train)\n",
        "print(\"Best parameters for Lasso Regression:\", random_search.best_params_)\n",
        "\n",
        "random_search = RandomizedSearchCV(ElasticNet(), param_distributions=param_distributions, n_iter=10)\n",
        "random_search.fit(X_train, y_train)\n",
        "print(\"Best parameters for ElasticNet Regression:\", random_search.best_params_)\n",
        "\n",
        "random_search = RandomizedSearchCV(LogisticRegression(), param_distributions=param_distributions, n_iter=10)\n",
        "random_search.fit(X_train, y_train)\n",
        "print(\"Best parameters for Logistic Regression:\", random_search.best_params_)\n",
        "\n",
        "random_search = RandomizedSearchCV(DecisionTreeClassifier(), param_distributions=tree_param_distributions, n_iter=10)\n",
        "random_search.fit(X_train, y_train)\n",
        "print(\"Best parameters for Decision Tree Classifier:\", random_search.best_params_)\n",
        "\n",
        "random_search = RandomizedSearchCV(DecisionTreeRegressor(), param_distributions=tree_param_distributions, n_iter=10)\n",
        "random_search.fit(X_train, y_train)\n",
        "print(\"Best parameters for Decision Tree Regressor:\", random_search.best_params_)\n",
        "\n",
        "random_search = RandomizedSearchCV(RandomForestClassifier(), param_distributions=tree_param_distributions, n_iter=10)\n",
        "random_search.fit(X_train, y_train)\n",
        "print(\"Best parameters for Random Forest Classifier:\", random_search.best_params_)\n",
        "\n",
        "random_search = RandomizedSearchCV(RandomForestRegressor(), param_distributions=tree_param_distributions, n_iter=10)\n",
        "random_search.fit(X_train, y_train)\n",
        "print(\"Best parameters for Random Forest Regressor:\", random_search.best_params_)\n",
        "\n",
        "random_search = RandomizedSearchCV(GradientBoostingClassifier(), param_distributions=tree_param_distributions, n_iter=10)\n",
        "random_search.fit(X_train, y_train)\n",
        "print(\"Best parameters for Gradient Boosting Classifier:\", random_search.best_params_)\n",
        "\n",
        "random_search = RandomizedSearchCV(GradientBoostingRegressor(), param_distributions=tree_param_distributions, n_iter=10)\n",
        "random_search.fit(X_train, y_train)\n",
        "print(\"Best parameters for Gradient Boosting Regressor:\", random_search.best_params_)\n",
        "\n",
        "random_search = RandomizedSearchCV(AdaBoostClassifier(), param_distributions=tree_param_distributions, n_iter=10)\n",
        "random_search.fit(X_train, y_train)\n",
        "print(\"Best parameters for AdaBoost Classifier:\", random_search.best_params_)\n",
        "\n",
        "random_search = RandomizedSearchCV(AdaBoostRegressor(), param_distributions=tree_param_distributions, n_iter=10)\n",
        "random_search.fit(X_train, y_train)\n",
        "print(\"Best parameters for AdaBoost Regressor:\", random_search.best_params_)\n",
        "\n",
        "random_search = RandomizedSearchCV(ExtraTreesClassifier(), param_distributions=tree_param_distributions, n_iter=10)\n",
        "random_search.fit(X_train, y_train)\n",
        "print(\"Best parameters for Extra Trees Classifier:\", random_search.best_params_)\n",
        "\n",
        "random_search = RandomizedSearchCV(ExtraTreesRegressor(), param_distributions=tree_param_distributions, n_iter=10)\n",
        "random_search.fit(X_train, y_train)\n",
        "print(\"Best parameters for Extra Trees Regressor:\", random_search.best_params_)\n",
        "\n",
        "random_search = RandomizedSearchCV(SVC(), param_distributions=svm_param_distributions, n_iter=10)\n",
        "random_search.fit(X_train, y_train)\n",
        "print(\"Best parameters for SVC:\", random_search.best_params_)\n",
        "\n",
        "random_search = RandomizedSearchCV(SVR(), param_distributions=svm_param_distributions, n_iter=10)\n",
        "random_search.fit(X_train, y_train)\n",
        "print(\"Best parameters for SVR:\", random_search.best_params_)\n",
        "\n",
        "random_search = RandomizedSearchCV(KMeans(), param_distributions=kmeans_param_distributions, n_iter=10)\n",
        "random_search.fit(X_train, y_train)\n",
        "print(\"Best parameters for KMeans:\", random_search.best_params_)\n",
        "\n",
        "random_search = RandomizedSearchCV(DBSCAN(), param_distributions={'eps': [0.1, 0.5, 1.0], 'min_samples': [5, 10, 20]}, n_iter=10)\n",
        "random_search.fit(X_train, y_train)\n",
        "print(\"Best parameters for DBSCAN:\", random_search.best_params_)\n",
        "\n",
        "random_search = RandomizedSearchCV(PCA(), param_distributions={'n_components': [2, 3, 4, 5]}, n_iter=10)\n",
        "random_search.fit(X_train, y_train)\n",
        "print(\"Best parameters for PCA:\", random_search.best_params_)\n",
        "\n",
        "random_search = RandomizedSearchCV(LinearDiscriminantAnalysis(), param_distributions={}, n_iter=10)\n",
        "random_search.fit(X_train, y_train)\n",
        "print(\"Best parameters for LinearDiscriminantAnalysis:\", random_search.best_params_)\n",
        "\n",
        "# Ensemble Methods\n",
        "# Voting Classifier and Voting Regressor do not have tunable hyperparameters.\n",
        "\n",
        "random_search = RandomizedSearchCV(BaggingClassifier(base_estimator=DecisionTreeClassifier()), param_distributions=tree_param_distributions, n_iter=10)\n",
        "random_search.fit(X_train, y_train)\n",
        "print(\"Best parameters for Bagging Classifier:\", random_search.best_params_)\n",
        "\n",
        "random_search = RandomizedSearchCV(BaggingRegressor(base_estimator=DecisionTreeRegressor()), param_distributions=tree_param_distributions, n_iter=10)\n",
        "random_search.fit(X_train, y_train)\n",
        "print(\"Best parameters for Bagging Regressor:\", random_search.best_params_)\n",
        "\n",
        "random_search = RandomizedSearchCV(StackingClassifier(estimators=[('dt', DecisionTreeClassifier()), ('rf', RandomForestClassifier()), ('svc', SVC())]), param_distributions=tree_param_distributions, n_iter=10)\n",
        "random_search.fit(X_train, y_train)\n",
        "print(\"Best parameters for Stacking Classifier:\", random_search.best_params_)\n",
        "\n",
        "random_search = RandomizedSearchCV(StackingRegressor(estimators=[('dt', DecisionTreeRegressor()), ('rf', RandomForestRegressor()), ('svr', SVR())]), param_distributions=tree_param_distributions, n_iter=10)\n",
        "random_search.fit(X_train, y_train)\n",
        "print(\"Best parameters for Stacking Regressor:\", random_search.best_params_)\n",
        "\n",
        "# Naive Bayes\n",
        "# GaussianNB and MultinomialNB do not have tunable hyperparameters.\n",
        "\n",
        "# K-Nearest Neighbors\n",
        "random_search = RandomizedSearchCV(KNeighborsClassifier(), param_distributions={'n_neighbors': [3, 5, 7], 'weights': ['uniform', 'distance'], 'algorithm': ['auto', 'ball_tree', 'kd_tree', 'brute']}, n_iter=10)\n",
        "random_search.fit(X_train, y_train)\n",
        "print(\"Best parameters for KNeighbors Classifier:\", random_search.best_params_)\n",
        "\n",
        "random_search = RandomizedSearchCV(KNeighborsRegressor(), param_distributions={'n_neighbors': [3, 5, 7], 'weights': ['uniform', 'distance'], 'algorithm': ['auto', 'ball_tree', 'kd_tree', 'brute']}, n_iter=10)\n",
        "random_search.fit(X_train, y_train)\n",
        "print(\"Best parameters for KNeighbors Regressor:\", random_search.best_params_)\n",
        "\n"
      ],
      "metadata": {
        "id": "y3YcLnOYPnuE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Bayesian Optimization"
      ],
      "metadata": {
        "id": "D8RcSv2lOVox"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "### example\n",
        "\n",
        "from skopt import BayesSearchCV\n",
        "from skopt.space import Real, Categorical, Integer\n",
        "param_space = {'C': Real(1e-6, 1e+6, prior='log-uniform'),\n",
        "               'kernel': Categorical(['linear', 'poly', 'rbf', 'sigmoid']),\n",
        "               'gamma': Real(1e-6, 1e+1, prior='log-uniform')}\n",
        "bayes_search = BayesSearchCV(estimator=model, search_spaces=param_space, n_iter=50, cv=5, scoring='accuracy', random_state=42)\n",
        "bayes_search.fit(X_train_scaled, y_train)\n",
        "print(\"Best parameters:\", bayes_search.best_params_)\n",
        "print(\"Best cross-validation accuracy:\", bayes_search.best_score_)"
      ],
      "metadata": {
        "id": "uAJ-JfkhOep8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Importing necessary libraries\n",
        "import numpy as np\n",
        "from sklearn.datasets import make_classification\n",
        "from sklearn.model_selection import cross_val_score\n",
        "from skopt import BayesSearchCV\n",
        "from skopt.space import Real, Categorical, Integer\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.linear_model import RidgeClassifier\n",
        "from sklearn.linear_model import Lasso\n",
        "from sklearn.linear_model import ElasticNet\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.ensemble import GradientBoostingClassifier\n",
        "from sklearn.ensemble import AdaBoostClassifier\n",
        "from sklearn.ensemble import ExtraTreesClassifier\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.cluster import KMeans\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
        "from sklearn.naive_bayes import GaussianNB\n",
        "from skopt import BayesSearchCV\n",
        "\n",
        "# Example data\n",
        "X, y = make_classification(n_samples=1000, n_features=20, n_classes=2, random_state=42)\n",
        "\n",
        "# Define the search space for hyperparameters for each model\n",
        "model_search_spaces = {\n",
        "    RandomForestClassifier: {\n",
        "        'n_estimators': Integer(10, 200),\n",
        "        'max_depth': Integer(2, 20),\n",
        "        'min_samples_split': Integer(2, 20),\n",
        "        'min_samples_leaf': Integer(1, 10),\n",
        "        'max_features': Real(0.1, 1.0, prior='uniform')\n",
        "    },\n",
        "    RidgeClassifier: {\n",
        "        'alpha': Real(1e-6, 1e6, prior='log-uniform')\n",
        "    },\n",
        "    Lasso: {\n",
        "        'alpha': Real(1e-6, 1e6, prior='log-uniform')\n",
        "    },\n",
        "    ElasticNet: {\n",
        "        'alpha': Real(1e-6, 1e6, prior='log-uniform'),\n",
        "        'l1_ratio': Real(0, 1, prior='uniform')\n",
        "    },\n",
        "    LogisticRegression: {\n",
        "        'C': Real(1e-6, 1e6, prior='log-uniform')\n",
        "    },\n",
        "    DecisionTreeClassifier: {\n",
        "        'max_depth': Integer(2, 20),\n",
        "        'min_samples_split': Integer(2, 20),\n",
        "        'min_samples_leaf': Integer(1, 10),\n",
        "        'max_features': Real(0.1, 1.0, prior='uniform')\n",
        "    },\n",
        "    GradientBoostingClassifier: {\n",
        "        'n_estimators': Integer(10, 200),\n",
        "        'learning_rate': Real(0.01, 1.0, prior='log-uniform'),\n",
        "        'max_depth': Integer(2, 20),\n",
        "        'min_samples_split': Integer(2, 20),\n",
        "        'min_samples_leaf': Integer(1, 10),\n",
        "        'max_features': Real(0.1, 1.0, prior='uniform')\n",
        "    },\n",
        "    AdaBoostClassifier: {\n",
        "        'n_estimators': Integer(10, 200),\n",
        "        'learning_rate': Real(0.01, 1.0, prior='log-uniform')\n",
        "    },\n",
        "    ExtraTreesClassifier: {\n",
        "        'n_estimators': Integer(10, 200),\n",
        "        'max_depth': Integer(2, 20),\n",
        "        'min_samples_split': Integer(2, 20),\n",
        "        'min_samples_leaf': Integer(1, 10),\n",
        "        'max_features': Real(0.1, 1.0, prior='uniform')\n",
        "    },\n",
        "    SVC: {\n",
        "        'C': Real(1e-6, 1e6, prior='log-uniform'),\n",
        "        'gamma': Real(1e-6, 1e1, prior='log-uniform')\n",
        "    },\n",
        "    KMeans: {\n",
        "        'n_clusters': Integer(2, 10),\n",
        "        'max_iter': Integer(100, 1000),\n",
        "    },\n",
        "    PCA: {\n",
        "        'n_components': Integer(2, 20)\n",
        "    },\n",
        "    LinearDiscriminantAnalysis: {},\n",
        "    GaussianNB: {}\n",
        "}\n",
        "\n",
        "# Create a dictionary to store optimized models\n",
        "optimized_models = {}\n",
        "\n",
        "# Loop through each model and perform Bayesian optimization\n",
        "for model, search_space in model_search_spaces.items():\n",
        "    opt = BayesSearchCV(\n",
        "        estimator=model(),\n",
        "        search_spaces=search_space,\n",
        "        n_iter=50,\n",
        "        cv=5,\n",
        "        scoring='accuracy',\n",
        "        random_state=42,\n",
        "        n_jobs=-1\n",
        "    )\n",
        "    opt.fit(X, y)\n",
        "    optimized_models[model.__name__] = {'best_params': opt.best_params_, 'best_score': opt.best_score_}\n",
        "\n",
        "# Print results\n",
        "for model_name, result in optimized_models.items():\n",
        "    print(f\"Model: {model_name}\")\n",
        "    print(f\"Best hyperparameters: {result['best_params']}\")\n",
        "    print(f\"Best score: {result['best_score']}\\n\")\n"
      ],
      "metadata": {
        "id": "BFARPdxRP6Da"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Optuna"
      ],
      "metadata": {
        "id": "ONrn86ksOYSJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "### example\n",
        "\n",
        "import optuna\n",
        "def objective(trial):\n",
        "    C = trial.suggest_loguniform('C', 1e-6, 1e+6)\n",
        "    kernel = trial.suggest_categorical('kernel', ['linear', 'poly', 'rbf', 'sigmoid'])\n",
        "    gamma = trial.suggest_loguniform('gamma', 1e-6, 1e+1)\n",
        "\n",
        "    model = SVC(C=C, kernel=kernel, gamma=gamma)\n",
        "    cv_score = cross_val_score(model, X_train_scaled, y_train, cv=5).mean()\n",
        "\n",
        "    return cv_score\n",
        "study = optuna.create_study(direction='maximize')\n",
        "study.optimize(objective, n_trials=100)\n",
        "best_params = study.best_params\n",
        "best_score = study.best_value\n",
        "print(\"Best parameters:\", best_params)\n",
        "print(\"Best cross-validation accuracy:\", best_score)"
      ],
      "metadata": {
        "id": "lMBQ77D_OhBG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import optuna\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import cross_val_score, train_test_split\n",
        "from sklearn.linear_model import LinearRegression, Ridge, Lasso, ElasticNet, LogisticRegression\n",
        "from sklearn.tree import DecisionTreeClassifier, DecisionTreeRegressor\n",
        "from sklearn.ensemble import RandomForestClassifier, RandomForestRegressor, GradientBoostingClassifier, GradientBoostingRegressor, AdaBoostClassifier, AdaBoostRegressor, ExtraTreesClassifier, ExtraTreesRegressor, VotingClassifier, VotingRegressor, BaggingClassifier, BaggingRegressor, StackingClassifier, StackingRegressor\n",
        "from sklearn.svm import SVC, SVR\n",
        "from sklearn.cluster import KMeans, DBSCAN\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
        "from sklearn.naive_bayes import GaussianNB, MultinomialNB\n",
        "from sklearn.neighbors import KNeighborsClassifier, KNeighborsRegressor\n",
        "\n",
        "# Example data\n",
        "X, y = np.random.randn(100, 5), np.random.randn(100)\n",
        "\n",
        "# Define objective function for Optuna\n",
        "def objective(trial):\n",
        "    # Define hyperparameters to be optimized for each model\n",
        "    if model_name.startswith('LinearRegression'):\n",
        "        alpha = trial.suggest_loguniform('alpha', 1e-5, 10)\n",
        "        model = LinearRegression(alpha=alpha)\n",
        "    elif model_name.startswith('Ridge'):\n",
        "        alpha = trial.suggest_loguniform('alpha', 1e-5, 10)\n",
        "        model = Ridge(alpha=alpha)\n",
        "    elif model_name.startswith('Lasso'):\n",
        "        alpha = trial.suggest_loguniform('alpha', 1e-5, 10)\n",
        "        model = Lasso(alpha=alpha)\n",
        "    elif model_name.startswith('ElasticNet'):\n",
        "        alpha = trial.suggest_loguniform('alpha', 1e-5, 10)\n",
        "        l1_ratio = trial.suggest_uniform('l1_ratio', 0, 1)\n",
        "        model = ElasticNet(alpha=alpha, l1_ratio=l1_ratio)\n",
        "    elif model_name.startswith('LogisticRegression'):\n",
        "        C = trial.suggest_loguniform('C', 1e-5, 100)\n",
        "        model = LogisticRegression(C=C)\n",
        "    elif model_name.startswith('DecisionTree'):\n",
        "        max_depth = trial.suggest_int('max_depth', 1, 32)\n",
        "        model = DecisionTreeClassifier(max_depth=max_depth) if 'Classifier' in model_name else DecisionTreeRegressor(max_depth=max_depth)\n",
        "    elif model_name.startswith('RandomForest'):\n",
        "        n_estimators = trial.suggest_int('n_estimators', 10, 100)\n",
        "        max_depth = trial.suggest_int('max_depth', 1, 32)\n",
        "        model = RandomForestClassifier(n_estimators=n_estimators, max_depth=max_depth) if 'Classifier' in model_name else RandomForestRegressor(n_estimators=n_estimators, max_depth=max_depth)\n",
        "    # Add other models and their hyperparameters here\n",
        "\n",
        "    # Evaluate the model using cross-validation\n",
        "    score = cross_val_score(model, X_train, y_train, cv=5).mean()\n",
        "    return score\n",
        "\n",
        "# Loop over each model\n",
        "for model_name, model_class in [\n",
        "    ('LinearRegression', LinearRegression),\n",
        "    ('Ridge', Ridge),\n",
        "    ('Lasso', Lasso),\n",
        "    ('ElasticNet', ElasticNet),\n",
        "    ('LogisticRegression', LogisticRegression),\n",
        "    ('DecisionTreeClassifier', DecisionTreeClassifier),\n",
        "    ('DecisionTreeRegressor', DecisionTreeRegressor),\n",
        "    ('RandomForestClassifier', RandomForestClassifier),\n",
        "    ('RandomForestRegressor', RandomForestRegressor),\n",
        "    # Add other models here\n",
        "]:\n",
        "    # Splitting data into training and testing sets\n",
        "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "    # Optimize hyperparameters using Optuna\n",
        "    study = optuna.create_study(direction='maximize')\n",
        "    study.optimize(objective, n_trials=100)\n",
        "\n",
        "    # Print the best hyperparameters and cross-validation score\n",
        "    print(f\"Best hyperparameters for {model_name}: {study.best_params}\")\n",
        "    print(f\"Best cross-validation score for {model_name}: {study.best_value}\")\n"
      ],
      "metadata": {
        "id": "n-1ZQxa7QStd"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}
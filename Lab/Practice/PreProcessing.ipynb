{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cgKQhpH4IvTe"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Assuming 'df' is your DataFrame\n",
        "# Checking for different scales using describe() function\n",
        "description = df.describe()\n",
        "print(description)\n",
        "\n",
        "## Inspect the range of values for each feature in the output. If the scales are vastly different, you may need to apply scaling techniques."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# For Pandas DataFrame\n",
        "null_values = df.isnull().sum()\n",
        "print(null_values)\n",
        "\n",
        "# For Scikit-learn\n",
        "import numpy as np\n",
        "# Assuming 'X' is your data\n",
        "missing_values = np.isnan(X).sum()\n",
        "print(missing_values)"
      ],
      "metadata": {
        "id": "YpRyPXOAJJmY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# For Pandas DataFrame\n",
        "# Remove rows with any null values\n",
        "df_cleaned = df.dropna()\n",
        "\n",
        "# Fill null values with a specific value\n",
        "df_filled = df.fillna(value)\n",
        "\n",
        "# For Scikit-learn\n",
        "from sklearn.impute import SimpleImputer\n",
        "\n",
        "imputer = SimpleImputer(strategy='mean')  # Other strategies: median, most_frequent\n",
        "X_imputed = imputer.fit_transform(X)\n"
      ],
      "metadata": {
        "id": "VjRKXQblJMaz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# For Pandas DataFrame\n",
        "# Assuming 'df' is your DataFrame\n",
        "categorical_columns = df.select_dtypes(include=['object']).columns\n",
        "print(categorical_columns)\n"
      ],
      "metadata": {
        "id": "H1lbnPE9JNq2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
        "\n",
        "# Standardization\n",
        "scaler = StandardScaler()\n",
        "X_scaled = scaler.fit_transform(X)\n",
        "\n",
        "# Normalization\n",
        "min_max_scaler = MinMaxScaler()\n",
        "X_normalized = min_max_scaler.fit_transform(X)\n",
        "\n",
        "## When features have different scales. It's essential for algorithms that are sensitive to feature scaling, like gradient descent-based algorithms. Data type: Numeric data."
      ],
      "metadata": {
        "id": "fj3cCir8JXzZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.preprocessing import OneHotEncoder\n",
        "\n",
        "# For Pandas DataFrame\n",
        "df_encoded = pd.get_dummies(df, columns=['categorical_column'])\n",
        "\n",
        "# For Scikit-learn\n",
        "encoder = OneHotEncoder()\n",
        "X_encoded = encoder.fit_transform(X)"
      ],
      "metadata": {
        "id": "PXKw_DsmJdrh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.preprocessing import LabelEncoder\n",
        "\n",
        "# Create an instance of LabelEncoder\n",
        "label_encoder = LabelEncoder()\n",
        "\n",
        "\n",
        "df['encoded_column'] = label_encoder.fit_transform(df['categorical_column'])\n",
        "#or\n",
        "X_encoded_label = label_encoder.fit_transform(X)"
      ],
      "metadata": {
        "id": "GkeLBdUpJ1rS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.preprocessing import MinMaxScaler, MaxAbsScaler\n",
        "\n",
        "scaler = MinMaxScaler()  # or MaxAbsScaler\n",
        "X_scaled = scaler.fit_transform(X)\n",
        "\n",
        "## When features have different ranges and need to be scaled to the same range.  Numeric data."
      ],
      "metadata": {
        "id": "pYroF4uzJsbR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.decomposition import PCA, TruncatedSVD\n",
        "\n",
        "pca = PCA(n_components=2)  # or TruncatedSVD\n",
        "X_reduced = pca.fit_transform(X)"
      ],
      "metadata": {
        "id": "am9j9E4yJxdX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Assuming 'df' is your DataFrame with a target column 'target_column'\n",
        "class_counts = df['target_column'].value_counts()\n",
        "print(class_counts)"
      ],
      "metadata": {
        "id": "HwiagGrbKP7J"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.utils import resample\n",
        "\n",
        "# Assuming 'df' is your DataFrame with a target column 'target_column'\n",
        "\n",
        "# Separate majority and minority classes\n",
        "majority_class = df[df['target_column'] == 'majority_class']\n",
        "minority_class = df[df['target_column'] == 'minority_class']\n",
        "\n",
        "# Oversample minority class\n",
        "minority_class_oversampled = resample(minority_class, replace=True, n_samples=len(majority_class))\n",
        "\n",
        "# Undersample majority class\n",
        "majority_class_undersampled = resample(majority_class, replace=False, n_samples=len(minority_class))\n",
        "\n",
        "# Combine oversampled minority class with majority class\n",
        "oversampled_df = pd.concat([majority_class, minority_class_oversampled])\n",
        "\n",
        "# Combine undersampled majority class with minority class\n",
        "undersampled_df = pd.concat([majority_class_undersampled, minority_class])"
      ],
      "metadata": {
        "id": "fRLOYbWtKkNF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from imblearn.over_sampling import SMOTE\n",
        "\n",
        "# Assuming 'X' is your feature matrix and 'y' is your target vector\n",
        "smote = SMOTE(random_state=42)\n",
        "X_resampled, y_resampled = smote.fit_resample(X, y)"
      ],
      "metadata": {
        "id": "cjIW1y9oKQ8X"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "# Assuming 'data' is your dataset\n",
        "# Z-score method\n",
        "def detect_outliers_zscore(data, threshold=3):\n",
        "    z_scores = np.abs((data - data.mean()) / data.std())\n",
        "    return np.where(z_scores > threshold)\n",
        "\n",
        "outliers_zscore_indices = detect_outliers_zscore(data)\n",
        "\n",
        "# IQR method\n",
        "def detect_outliers_iqr(data, threshold=1.5):\n",
        "    Q1 = np.percentile(data, 25)\n",
        "    Q3 = np.percentile(data, 75)\n",
        "    IQR = Q3 - Q1\n",
        "    lower_bound = Q1 - (threshold * IQR)\n",
        "    upper_bound = Q3 + (threshold * IQR)\n",
        "    return np.where((data < lower_bound) | (data > upper_bound))\n",
        "\n",
        "outliers_iqr_indices = detect_outliers_iqr(data)\n"
      ],
      "metadata": {
        "id": "FRaCUmbSKxU_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.cluster import KMeans\n",
        "\n",
        "# Assuming 'data' is your dataset\n",
        "def detect_outliers_cluster(data, n_clusters=3):\n",
        "    kmeans = KMeans(n_clusters=n_clusters)\n",
        "    kmeans.fit(data)\n",
        "    cluster_centers = kmeans.cluster_centers_\n",
        "    distances = np.linalg.norm(data - cluster_centers[kmeans.labels_], axis=1)\n",
        "    threshold = np.percentile(distances, 95)  # Adjust percentile as needed\n",
        "    return np.where(distances > threshold)\n",
        "\n",
        "outliers_cluster_indices = detect_outliers_cluster(data)\n"
      ],
      "metadata": {
        "id": "AaOQm1KjKx72"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Removing outliers\n",
        "cleaned_data_zscore = data[(np.abs((data - data.mean()) / data.std()) < threshold).all(axis=1)]\n",
        "cleaned_data_iqr = data[~((data < lower_bound) | (data > upper_bound)).any(axis=1)]\n",
        "cleaned_data_cluster = np.delete(data, outliers_cluster_indices, axis=0)\n",
        "\n",
        "# Imputing outliers with mean or median\n",
        "data_zscore_imputed = np.where((np.abs((data - data.mean()) / data.std()) > threshold), np.nan, data)\n",
        "data_zscore_imputed = np.where(np.isnan(data_zscore_imputed), np.nanmean(data_zscore_imputed, axis=0), data_zscore_imputed)\n",
        "\n",
        "data_iqr_imputed = np.where(((data < lower_bound) | (data > upper_bound)), np.nan, data)\n",
        "data_iqr_imputed = np.where(np.isnan(data_iqr_imputed), np.nanmean(data_iqr_imputed, axis=0), data_iqr_imputed)\n",
        "\n",
        "# Replace outliers with mean or median\n",
        "data_cluster_imputed = data.copy()\n",
        "for index in outliers_cluster_indices:\n",
        "    data_cluster_imputed[index] = np.nanmedian(data_cluster_imputed, axis=0)\n"
      ],
      "metadata": {
        "id": "MH0sIRVXK0nF"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}
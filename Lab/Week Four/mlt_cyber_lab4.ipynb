{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Lab 4 (30-01-2024)\n",
        "\n",
        "This lab experiments help you master how to do linear regression and multiple linear regression.\n",
        "\n",
        "We will be using real estate database provided in lab2.\n",
        "\n"
      ],
      "metadata": {
        "id": "-6H1OBnEnRg5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "Registration_Number = \"22011103020\"\n",
        "Name = \"Jayashre\"\n",
        "\n",
        "# Python Program to Get IP Address\n",
        "import socket\n",
        "hostname = socket.gethostname()\n",
        "IPAddr = socket.gethostbyname(hostname)\n",
        "\n",
        "print(\"My name is \" + Name + \" and my roll no : \" + Registration_Number)\n",
        "print(\"Computer IP Address is: \" + IPAddr)"
      ],
      "metadata": {
        "id": "8YQTqvOLYMMY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Experiment 1 - Predicting diabetes for a person using Linear Regression\n",
        "Load diabetes dataset\n",
        " Split the dataset into\n",
        "\n",
        "1.  Split the dataset into train (90%) and test (10%) using scikit learn\n",
        "2.  Fill the cost function\n",
        "3.  Fill the liner regression fit function\n",
        "4.  Fill the routine for Gradient descent\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "sJLIrF7ooubF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "ByNm9jc-1qSs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.datasets import load_diabetes\n",
        "\n",
        "dataset = load_diabetes(as_frame=True,scaled=True)\n",
        "\n",
        "dataset_dia = dataset.data\n",
        "\n",
        "re_df = dataset_dia.copy()"
      ],
      "metadata": {
        "id": "u8AI5tO34-8M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(re_df.columns)"
      ],
      "metadata": {
        "id": "8Cyj9VBq5nib"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(dataset.target)"
      ],
      "metadata": {
        "id": "VZWWYildEakN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "categorical_columns = re_df.select_dtypes(include=['object']).columns\n",
        "numerical_columns = re_df.select_dtypes(exclude=['object']).columns\n",
        "\n",
        "print(categorical_columns)\n",
        "print(\"-------------------\")\n",
        "print(numerical_columns)"
      ],
      "metadata": {
        "id": "_vhiS0og50nC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "import numpy as np\n",
        "\n",
        "X = re_df['age'].to_numpy()\n",
        "Y = dataset.target.to_numpy()\n",
        "\n",
        "# split the dataset into test and train\n",
        "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.1, random_state=42)"
      ],
      "metadata": {
        "id": "tc4VEc2v1nY0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Experiment 1 - Linear regression with one variable\n",
        "# Fill following functions\n",
        "def linear_reg_function(X,w,b):\n",
        "  \"\"\"\n",
        "  Model function for the\n",
        "  X is the feature vector\n",
        "  Y is the target vector \"For example: \"\n",
        "  b is the bias\n",
        "  w is the weight\n",
        "  \"\"\"\n",
        "\n",
        "  return w*X + b"
      ],
      "metadata": {
        "id": "bSbmInIl68Vz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Experiment 1 - Linear regression with one variable\n",
        "# Fill following functions\n",
        "\n",
        "def cost_function(X,w,b,Y):\n",
        "  \"\"\"\n",
        "  Cost function for the linear regression\n",
        "  This is the function that will be minimised using gradient descent\n",
        "  X is the feature vector \"For example: Area of the house\"\n",
        "  Y is the target vector \"For example: Price of the house\"\n",
        "  b is the bias\n",
        "  w is the weight\n",
        "  \"\"\"\n",
        "  cf_val = 0\n",
        "  m = len(Y)\n",
        "  for i in range(0,m):\n",
        "    cf_val += (linear_reg_function(X[i], w, b) - Y[i]) ** 2\n",
        "  cf_val /= (2*m)\n",
        "  # Compute the cf_val = sum across all i : (wX_i + b)^2\n",
        "\n",
        "  return cf_val"
      ],
      "metadata": {
        "id": "jP2-Fnxx-B_i"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Experiment 1 - Linear regression with one variable\n",
        "# Fill following functions\n",
        "\n",
        "def cost_function_with_reg(X,w,b,Y, gamma=0.1):\n",
        "  \"\"\"\n",
        "  Cost function for the linear regression\n",
        "  This is the function that will be minimised using gradient descent\n",
        "  X is the feature vector \"For example: Area of the house\"\n",
        "  Y is the target vector \"For example: Price of the house\"\n",
        "  b is the bias\n",
        "  w is the weight\n",
        "  \"\"\"\n",
        "  cf_val = 0\n",
        "  m = len(Y)\n",
        "  for i in range(0,m):\n",
        "    cf_val += (linear_reg_function(X[i], w, b) - Y[i]) ** 2\n",
        "  cf_val /= (2*m) + gamma*(w**2 + b**2)\n",
        "  # Compute the cf_val = sum across all i : (wX_i + b)^2\n",
        "\n",
        "  return cf_val"
      ],
      "metadata": {
        "id": "OFFECVAcHMCU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Experiment 1 - Linear regression with one variable\n",
        "# Fill following functions\n",
        "def gradient_function(X,w,b,Y):\n",
        "  \"\"\"\n",
        "  Gradient function for the linear regression with one variable\n",
        "  This is the function that will be used to get the gradient in gradient descent\n",
        "  X is the feature vector \"For example: Area of the house\"\n",
        "  Y is the target vector \"For example: Price of the house\"\n",
        "  b is the bias\n",
        "  w is the weight\n",
        "  \"\"\"\n",
        "  grad_val_w = 0.0\n",
        "  grad_val_b = 0.0\n",
        "  m = len(Y)\n",
        "\n",
        "  for i in range(0,m):\n",
        "    grad_val_w += (linear_reg_function(X[i], w, b) - Y[i])*X[i]\n",
        "    grad_val_b += (linear_reg_function(X[i], w, b) - Y[i])\n",
        "  grad_val_w /= (m)\n",
        "  grad_val_b /= (m)\n",
        "  # Compute the grad_val_w, grad_val_b for the cost function\n",
        "\n",
        "  return grad_val_w, grad_val_b"
      ],
      "metadata": {
        "id": "Q5GUdXEb45LG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Experiment 1 - Linear regression with one variable\n",
        "# Fill following functions\n",
        "def gradient_function_with_reg(X,w,b,Y, gamma=0.1):\n",
        "  \"\"\"\n",
        "  Gradient function for the linear regression with one variable\n",
        "  This is the function that will be used to get the gradient in gradient descent\n",
        "  X is the feature vector \"For example: Area of the house\"\n",
        "  Y is the target vector \"For example: Price of the house\"\n",
        "  b is the bias\n",
        "  w is the weight\n",
        "  \"\"\"\n",
        "  grad_val_w = 0.0\n",
        "  grad_val_b = 0.0\n",
        "  m = len(Y)\n",
        "\n",
        "  for i in range(0,m):\n",
        "    grad_val_w += (linear_reg_function(X[i], w, b) - Y[i])*X[i]\n",
        "    grad_val_b += (linear_reg_function(X[i], w, b) - Y[i])\n",
        "  grad_val_w /= ((m) + 2*gamma*w)\n",
        "  grad_val_b /= ((m) + 2*gamma*b)\n",
        "  # Compute the grad_val_w, grad_val_b for the cost function\n",
        "\n",
        "  return grad_val_w, grad_val_b"
      ],
      "metadata": {
        "id": "R1wNODBYHU19"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Experiment 1 - Linear regression with one variable\n",
        "# Fill following functions\n",
        "\n",
        "# the below function is a template you may edit it\n",
        "def gradient_descent_linear_reg_one_variable(X,Y, gradient, learning_rate, bias=1.0, weight=1.0, converge_param=0.1):\n",
        "  w = weight\n",
        "  b = bias\n",
        "  isConverged = False\n",
        "  num_steps = 1\n",
        "  while (not isConverged):\n",
        "    num_steps +=1\n",
        "    #cs_val = model_func(X,w,b,Y)\n",
        "    dw, db = gradient(X,w,b,Y)\n",
        "    w -= learning_rate*dw\n",
        "    b -= learning_rate*db\n",
        "    if abs(learning_rate * dw) < converge_param and abs(learning_rate * db) < converge_param:\n",
        "      isConverged = True\n",
        "    weight = w\n",
        "    bias = b\n",
        "  print(\"Converged in \" , num_steps,  \"iterations...\")\n",
        "  return w, b"
      ],
      "metadata": {
        "id": "TyeQUBDbQ6Nl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4MOPlLH4m-Vz"
      },
      "outputs": [],
      "source": [
        "# Test with different learning rates and start values and see how gradient descent works\n",
        "print(\"Linear Regression without Regularization\")\n",
        "print(\"---------------------------------------------\")\n",
        "\n",
        "print(\"Using Given Learning Rate\")\n",
        "learning_rate_given = 0.9\n",
        "\n",
        "print(\"Using X as the feature and Y as the target\")\n",
        "w_g,b_g = gradient_descent_linear_reg_one_variable(X, Y, gradient_function, learning_rate_given)\n",
        "print(\"Weights:\", w_g)\n",
        "print(\"Bias:\", b_g)\n",
        "print(\"---------------\")\n",
        "cost_g = cost_function(X, w_g, b_g, Y)\n",
        "print(\"Cost:\", cost_g)\n",
        "\n",
        "print(\"---------------------------------------------\")\n",
        "\n",
        "print(\"Using X_train as the feature and y_train as the target\")\n",
        "w_t_g,b_t_g = gradient_descent_linear_reg_one_variable(X_train, Y_train, gradient_function, learning_rate_given)\n",
        "print(\"Weights:\", w_t_g)\n",
        "print(\"Bias:\", b_t_g)\n",
        "print(\"---------------\")\n",
        "cost_g_t = cost_function(X_train, w_t_g, b_t_g, Y_train)\n",
        "print(\"Cost:\", cost_g_t)\n",
        "print(\"---------------------------------------------\")\n",
        "\n",
        "print(\"Using Ideal Learning Rate\")\n",
        "learning_rate_ideal = 0.01\n",
        "\n",
        "print(\"Using X as the feature and Y as the target\")\n",
        "w_i,b_i = gradient_descent_linear_reg_one_variable(X, Y, gradient_function, learning_rate_ideal)\n",
        "print(\"Weights:\", w_i)\n",
        "print(\"Bias:\", b_i)\n",
        "print(\"---------------\")\n",
        "cost_i = cost_function(X, w_i, b_i, Y)\n",
        "print(\"Cost:\", cost_i)\n",
        "print(\"---------------------------------------------\")\n",
        "\n",
        "print(\"Using X_train as the feature and y_train as the target\")\n",
        "w_t_i,b_t_i = gradient_descent_linear_reg_one_variable(X_train, Y_train, gradient_function, learning_rate_ideal)\n",
        "print(\"Weights:\", w_t_g)\n",
        "print(\"Bias:\", b_t_g)\n",
        "print(\"---------------\")\n",
        "cost_i_t = cost_function(X_train, w_t_g, b_t_g, Y_train)\n",
        "print(\"Cost:\", cost_i_t)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# once you have found out the optimal w,b plot the gradient function for [w-100,w+100] and [b-100,b+100]\n",
        "# the shape of the curve is important in the convergence of the gradient descent\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "plt.plot(X, Y, 'o')\n",
        "zx = np.linspace(-0.1, 0.1, 100)\n",
        "zy = linear_reg_function(zx,w_g,b_g)\n",
        "plt.plot(zx,zy)\n",
        "plt.show()\n",
        "\n",
        "\n",
        "print(\"---------------------------------------------\")\n",
        "\n",
        "plt.plot(X_train, Y_train, 'o')\n",
        "zx_t = np.linspace(-0.1, 0.1, 100)\n",
        "zy_t = linear_reg_function(zx_t,w_t_g, b_t_g)\n",
        "plt.plot(zx_t,zy_t)\n",
        "plt.show()\n",
        "\n",
        "print(\"-----------------------------------------------------------------------------------\")\n",
        "\n",
        "plt.plot(X, Y, 'o')\n",
        "zx = np.linspace(-0.1, 0.1, 100)\n",
        "zy = linear_reg_function(zx,w_i,b_i)\n",
        "plt.plot(zx,zy)\n",
        "plt.show()\n",
        "\n",
        "\n",
        "print(\"---------------------------------------------\")\n",
        "\n",
        "plt.plot(X_train, Y_train, 'o')\n",
        "zx_t = np.linspace(-0.1, 0.1, 100)\n",
        "zy_t = linear_reg_function(zx_t,w_t_i,b_t_i)\n",
        "plt.plot(zx_t,zy_t)\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "ncJ_p0WCRU1C"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Linear Regression with Regularization\")\n",
        "print(\"---------------------------------------------\")\n",
        "\n",
        "print(\"Using Given Learning Rate\")\n",
        "learning_rate_given = 0.9\n",
        "\n",
        "print(\"Using X as the feature and Y as the target\")\n",
        "w_g,b_g = gradient_descent_linear_reg_one_variable(X, Y, gradient_function_with_reg, learning_rate_given)\n",
        "print(\"Weights:\", w_g)\n",
        "print(\"Bias:\", b_g)\n",
        "print(\"---------------\")\n",
        "cost_g = cost_function_with_reg(X, w_g, b_g, Y)\n",
        "print(\"Cost:\", cost_g)\n",
        "\n",
        "print(\"---------------------------------------------\")\n",
        "\n",
        "print(\"Using X_train as the feature and y_train as the target\")\n",
        "w_t_g,b_t_g = gradient_descent_linear_reg_one_variable(X_train, Y_train, gradient_function_with_reg, learning_rate_given)\n",
        "print(\"Weights:\", w_t_g)\n",
        "print(\"Bias:\", b_t_g)\n",
        "print(\"---------------\")\n",
        "cost_g_t = cost_function_with_reg(X_train, w_t_g, b_t_g, Y_train)\n",
        "print(\"Cost:\", cost_g_t)\n",
        "print(\"---------------------------------------------\")\n",
        "\n",
        "print(\"Using Ideal Learning Rate\")\n",
        "learning_rate_ideal = 0.01\n",
        "\n",
        "print(\"Using X as the feature and Y as the target\")\n",
        "w_i,b_i = gradient_descent_linear_reg_one_variable(X, Y, gradient_function_with_reg, learning_rate_ideal)\n",
        "print(\"Weights:\", w_i)\n",
        "print(\"Bias:\", b_i)\n",
        "print(\"---------------\")\n",
        "cost_i = cost_function_with_reg(X, w_i, b_i, Y)\n",
        "print(\"Cost:\", cost_i)\n",
        "print(\"---------------------------------------------\")\n",
        "\n",
        "print(\"Using X_train as the feature and y_train as the target\")\n",
        "w_t_i,b_t_i = gradient_descent_linear_reg_one_variable(X_train, Y_train, gradient_function_with_reg, learning_rate_ideal)\n",
        "print(\"Weights:\", w_t_g)\n",
        "print(\"Bias:\", b_t_g)\n",
        "print(\"---------------\")\n",
        "cost_i_t = cost_function_with_reg(X_train, w_t_g, b_t_g, Y_train)\n",
        "print(\"Cost:\", cost_i_t)"
      ],
      "metadata": {
        "id": "AXsuCijlRDnt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# once you have found out the optimal w,b plot the gradient function for [w-100,w+100] and [b-100,b+100]\n",
        "# the shape of the curve is important in the convergence of the gradient descent\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "plt.plot(X, Y, 'o')\n",
        "zx = np.linspace(-0.1, 0.1, 100)\n",
        "zy = linear_reg_function(zx,w_g,b_g)\n",
        "plt.plot(zx,zy)\n",
        "plt.show()\n",
        "\n",
        "\n",
        "print(\"---------------------------------------------\")\n",
        "\n",
        "plt.plot(X_train, Y_train, 'o')\n",
        "zx_t = np.linspace(-0.1, 0.1, 100)\n",
        "zy_t = linear_reg_function(zx_t,w_t_g, b_t_g)\n",
        "plt.plot(zx_t,zy_t)\n",
        "plt.show()\n",
        "\n",
        "print(\"-----------------------------------------------------------------------------------\")\n",
        "\n",
        "plt.plot(X, Y, 'o')\n",
        "zx = np.linspace(-0.1, 0.1, 100)\n",
        "zy = linear_reg_function(zx,w_i,b_i)\n",
        "plt.plot(zx,zy)\n",
        "plt.show()\n",
        "\n",
        "\n",
        "print(\"---------------------------------------------\")\n",
        "\n",
        "plt.plot(X_train, Y_train, 'o')\n",
        "zx_t = np.linspace(-0.1, 0.1, 100)\n",
        "zy_t = linear_reg_function(zx_t,w_t_i,b_t_i)\n",
        "plt.plot(zx_t,zy_t)\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "QNDTrxNM8lm0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Experminent 2 - Multiple linear regression\n",
        "use more features and modify the code for more than 1 features"
      ],
      "metadata": {
        "id": "TSjBHV8t8c63"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "X_ml = re_df[['age','bmi']].to_numpy()\n",
        "Y_ml = dataset.target.to_numpy()\n",
        "\n",
        "# split the dataset into test and train\n",
        "X_train_ml, X_test_ml, Y_train_ml, Y_test_ml = train_test_split(X_ml, Y_ml, test_size=0.1, random_state=42)"
      ],
      "metadata": {
        "id": "kvyG48GnEXEC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Experiment 2 - Linear regression with multi variable\n",
        "# Fill following functions\n",
        "def linear_reg_function_multiple(X,w,b):\n",
        "  \"\"\"\n",
        "  Model function for the multiple linear regression\n",
        "  X is the feature Matrix \"For example: f features and m samples\"\n",
        "  Y is the target vector \"For example: Price of the house\"\n",
        "  b is the bias\n",
        "  w is the weight vector with length = f-12.892459443801385 141.60304608176983\n",
        "  \"\"\"\n",
        "\n",
        "  return X*w + b"
      ],
      "metadata": {
        "id": "h0I2Mzj2GOuO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Experiment 2 - Linear regression with multi variable\n",
        "# Fill following functions\n",
        "\n",
        "def cost_function_multiple(X,w,b,Y):\n",
        "  \"\"\"\n",
        "  Cost function for the linear regression\n",
        "  This is the function that will be minimised using gradient descent\n",
        "  X is the feature Matrix \"For example: f features and m samples\"\n",
        "  Y is the target vector \"For example: Price of the house\"\n",
        "  b is the bias\n",
        "  w is the weight vector with length = f-1\n",
        "  \"\"\"\n",
        "  cf_val = 0.0\n",
        "  # Compute the cf_val = sum across all i : (wX_i + b)^2\n",
        "  m = len(Y)\n",
        "  for i in range(0,m):\n",
        "    cf_val += (linear_reg_function_multiple(X[i], w, b) - Y[i]) ** 2\n",
        "  cf_val /= (2*m)\n",
        "  return cf_val"
      ],
      "metadata": {
        "id": "VUQu07N-SOZO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Experiment 2 - Linear regression with multi variable\n",
        "# Fill following functions\n",
        "\n",
        "def cost_function_multiple_reg(X,w,b,Y, gamma=0.1):\n",
        "  \"\"\"\n",
        "  Cost function for the linear regression\n",
        "  This is the function that will be minimised using gradient descent\n",
        "  X is the feature Matrix \"For example: f features and m samples\"\n",
        "  Y is the target vector \"For example: Price of the house\"\n",
        "  b is the bias\n",
        "  w is the weight vector with length = f-1\n",
        "  \"\"\"\n",
        "  cf_val = 0.0\n",
        "  # Compute the cf_val = sum across all i : (wX_i + b)^2\n",
        "  m = len(Y)\n",
        "  for i in range(0,m):\n",
        "    cf_val += (linear_reg_function_multiple(X[i], w, b) - Y[i]) ** 2\n",
        "  cf_val /= (2*m) + gamma*(w**2 + b**2)\n",
        "  return cf_val"
      ],
      "metadata": {
        "id": "J-sXcRasSluE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Experiment 2 - Linear regression with multi variable\n",
        "# Fill following functions\n",
        "\n",
        "def gradient_function_multiple(X,w,b,Y):\n",
        "  \"\"\"\n",
        "  Gradient function for the linear regression with one variable\n",
        "  This is the function that will be used to get the gradient in gradient descent\n",
        "  X is the feature Matrix \"For example: f features and m samples\"\n",
        "  Y is the target vector \"For example: Price of the house\"\n",
        "  b is the bias\n",
        "  w is the weight vector with length = f-1\n",
        "  \"\"\"\n",
        "\n",
        "  grad_val_w = 0.0\n",
        "  grad_val_b = 0.0\n",
        "  m = len(Y)\n",
        "\n",
        "  for i in range(0,m):\n",
        "    grad_val_w += (linear_reg_function_multiple(X[i], w, b) - Y[i])*X[i]\n",
        "    grad_val_b += (linear_reg_function_multiple(X[i], w, b) - Y[i])\n",
        "  grad_val_w /= (m)\n",
        "  grad_val_b /= (m)\n",
        "  # Compute the grad_val_w, grad_val_b for the cost function\n",
        "\n",
        "  return grad_val_w, grad_val_b"
      ],
      "metadata": {
        "id": "ZxwSWQLCSz4r"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Experiment 2 - Linear regression with multi variable\n",
        "# Fill following functions\n",
        "\n",
        "def gradient_function_multiple_reg(X,w,b,Y, gamma=0.1):\n",
        "  \"\"\"\n",
        "  Gradient function for the linear regression with one variable\n",
        "  This is the function that will be used to get the gradient in gradient descent\n",
        "  X is the feature Matrix \"For example: f features and m samples\"\n",
        "  Y is the target vector \"For example: Price of the house\"\n",
        "  b is the bias\n",
        "  w is the weight vector with length = f-1\n",
        "  \"\"\"\n",
        "\n",
        "  grad_val_w = 0.0\n",
        "  grad_val_b = 0.0\n",
        "  m = len(Y)\n",
        "\n",
        "  for i in range(0,m):\n",
        "    grad_val_w += (linear_reg_function_multiple(X[i], w, b) - Y[i])*X[i]\n",
        "    grad_val_b += (linear_reg_function_multiple(X[i], w, b) - Y[i])\n",
        "  grad_val_w /= ((m) + 2*gamma*w)\n",
        "  grad_val_b /= ((m) + 2*gamma*b)\n",
        "  # Compute the grad_val_w, grad_val_b for the cost function\n",
        "\n",
        "  return grad_val_w, grad_val_b"
      ],
      "metadata": {
        "id": "3YTTolP9S0mu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Experiment 2 - Linear regression with multi variable\n",
        "# Fill following functions\n",
        "\n",
        "# the below function is a template you may edit it\n",
        "def gradient_descent_linear_reg_multi_variable(X,Y, gradient, learning_rate, bias=1.0, weight=1.0, converge_param=0.001):\n",
        "# Initialise w,b\n",
        "  w = weight\n",
        "  b = bias\n",
        "  isConverged = False\n",
        "  num_steps = 1\n",
        "  while (not isConverged):\n",
        "    num_steps +=1\n",
        "    #cs_val = model_func(X,w,b,Y)\n",
        "    dw, db = gradient(X,w,b,Y)\n",
        "    w -= learning_rate*dw\n",
        "    b -= learning_rate*db\n",
        "    if all(abs(learning_rate * dw_i) < converge_param and abs(learning_rate * db_i) < converge_param for dw_i, db_i in zip(dw, db)):\n",
        "      isConverged = True\n",
        "    weight = w\n",
        "    bias = b\n",
        "  print(\"Converged in \" , num_steps,  \"iterations...\")\n",
        "  return w, b\n",
        "\n",
        "# Similar to previous case, Test with different learning rates and start values and see how gradient descent works\n",
        "\n"
      ],
      "metadata": {
        "id": "DG8wpaJb_sFx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Test with different learning rates and start values and see how gradient descent works\n",
        "print(\"Multiple Linear Regression without Regularization\")\n",
        "print(\"---------------------------------------------\")\n",
        "\n",
        "print(\"Using Given Learning Rate\")\n",
        "learning_rate_given = 0.9\n",
        "\n",
        "print(\"Using X as the feature and Y as the target\")\n",
        "w_g,b_g = gradient_descent_linear_reg_multi_variable(X_ml, Y_ml, gradient_function_multiple, learning_rate_given)\n",
        "print(\"Weights:\", w_g)\n",
        "print(\"Bias:\", b_g)\n",
        "print(\"---------------\")\n",
        "cost_g = cost_function_multiple(X, w_g, b_g, Y)\n",
        "print(\"Cost:\", cost_g)\n",
        "\n",
        "print(\"---------------------------------------------\")\n",
        "\n",
        "print(\"Using X_train as the feature and y_train as the target\")\n",
        "w_t_g,b_t_g = gradient_descent_linear_reg_multi_variable(X_train_ml, Y_train_ml, gradient_function_multiple, learning_rate_given)\n",
        "print(\"Weights:\", w_t_g)\n",
        "print(\"Bias:\", b_t_g)\n",
        "print(\"---------------\")\n",
        "cost_g_t = cost_function_multiple(X_train, w_t_g, b_t_g, Y_train)\n",
        "print(\"Cost:\", cost_g_t)\n",
        "print(\"---------------------------------------------\")\n",
        "\n",
        "print(\"Using Ideal Learning Rate\")\n",
        "learning_rate_ideal = 0.5\n",
        "\n",
        "print(\"Using X as the feature and Y as the target\")\n",
        "w_i,b_i = gradient_descent_linear_reg_multi_variable(X_ml, Y_ml, gradient_function_multiple, learning_rate_ideal)\n",
        "print(\"Weights:\", w_i)\n",
        "print(\"Bias:\", b_i)\n",
        "print(\"---------------\")\n",
        "cost_i = cost_function_multiple(X, w_i, b_i, Y)\n",
        "print(\"Cost:\", cost_i)\n",
        "print(\"---------------------------------------------\")\n",
        "\n",
        "print(\"Using X_train as the feature and y_train as the target\")\n",
        "w_t_i,b_t_i = gradient_descent_linear_reg_multi_variable(X_train_ml, Y_train_ml, gradient_function_multiple, learning_rate_ideal)\n",
        "print(\"Weights:\", w_t_i)\n",
        "print(\"Bias:\", b_t_i)\n",
        "print(\"---------------\")\n",
        "cost_i_t = cost_function_multiple(X_train, w_t_i, b_t_i, Y_train)\n",
        "print(\"Cost:\", cost_i_t)\n"
      ],
      "metadata": {
        "id": "Fm-pjjm0USUR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# once you have found out the optimal w,b plot the gradient function for [w-100,w+100] and [b-100,b+100]\n",
        "# the shape of the curve is important in the convergence of the gradient descent\n",
        "\n",
        "for i in range(len(w_g)):\n",
        "    plt.plot(X_ml[:, i], Y_ml, 'o', label=f'Data {i+1}')\n",
        "\n",
        "    zx_ml = np.linspace(np.min(X_ml[:, i]) - 0.1, np.max(X_ml[:, i]) + 0.1, 100)\n",
        "    zy_ml = linear_reg_function_multiple(zx_ml, w_g[i], b_g[i])\n",
        "    plt.plot(zx_ml, zy_ml, label=f'Line {i+1}')\n",
        "\n",
        "plt.legend()\n",
        "plt.show()\n",
        "\n",
        "\n",
        "print(\"---------------------------------------------\")\n",
        "\n",
        "for i in range(len(w_t_g)):\n",
        "    plt.plot(X_train_ml[:, i], Y_train_ml, 'o', label=f'Data {i+1}')\n",
        "\n",
        "    zx_t_ml = np.linspace(np.min(X_train_ml[:, i]) - 0.1, np.max(X_train_ml[:, i]) + 0.1, 100)\n",
        "    zy_t_ml = linear_reg_function_multiple(zx_t_ml, w_t_g[i], b_t_g[i])\n",
        "    plt.plot(zx_t_ml, zy_t_ml, label=f'Line {i+1}')\n",
        "\n",
        "plt.legend()\n",
        "plt.show()\n",
        "\n",
        "print(\"-----------------------------------------------------------------------------------\")\n",
        "\n",
        "for i in range(len(w_i)):\n",
        "    plt.plot(X_ml[:, i], Y_ml, 'o', label=f'Data {i+1}')\n",
        "\n",
        "    zx_ml = np.linspace(np.min(X_ml[:, i]) - 0.1, np.max(X_ml[:, i]) + 0.1, 100)\n",
        "    zy_ml = linear_reg_function_multiple(zx_ml, w_i[i], b_i[i])\n",
        "    plt.plot(zx_ml, zy_ml, label=f'Line {i+1}')\n",
        "\n",
        "plt.legend()\n",
        "plt.show()\n",
        "\n",
        "\n",
        "print(\"---------------------------------------------\")\n",
        "\n",
        "\n",
        "for i in range(len(w_t_i)):\n",
        "    plt.plot(X_train_ml[:, i], Y_train_ml, 'o', label=f'Data {i+1}')\n",
        "\n",
        "    zx_t_ml = np.linspace(np.min(X_train_ml[:, i]) - 0.1, np.max(X_train_ml[:, i]) + 0.1, 100)\n",
        "    zy_t_ml = linear_reg_function_multiple(zx_t_ml, w_t_i[i], b_t_i[i])\n",
        "    plt.plot(zx_t_ml, zy_t_ml, label=f'Line {i+1}')\n",
        "\n",
        "plt.legend()\n",
        "plt.show()\n",
        "\n"
      ],
      "metadata": {
        "id": "CU9zgxADChaM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Test with different learning rates and start values and see how gradient descent works\n",
        "print(\"Multiple Linear Regression with Regularization\")\n",
        "print(\"---------------------------------------------\")\n",
        "\n",
        "print(\"Using Given Learning Rate\")\n",
        "learning_rate_given = 0.9\n",
        "\n",
        "print(\"Using X as the feature and Y as the target\")\n",
        "w_g,b_g = gradient_descent_linear_reg_multi_variable(X_ml, Y_ml, gradient_function_multiple_reg, learning_rate_given)\n",
        "print(\"Weights:\", w_g)\n",
        "print(\"Bias:\", b_g)\n",
        "print(\"---------------\")\n",
        "cost_g = cost_function_multiple_reg(X, w_g, b_g, Y)\n",
        "print(\"Cost:\", cost_g)\n",
        "\n",
        "print(\"---------------------------------------------\")\n",
        "\n",
        "print(\"Using X_train as the feature and y_train as the target\")\n",
        "w_t_g,b_t_g = gradient_descent_linear_reg_multi_variable(X_train_ml, Y_train_ml, gradient_function_multiple_reg, learning_rate_given)\n",
        "print(\"Weights:\", w_t_g)\n",
        "print(\"Bias:\", b_t_g)\n",
        "print(\"---------------\")\n",
        "cost_g_t = cost_function_multiple_reg(X_train, w_t_g, b_t_g, Y_train)\n",
        "print(\"Cost:\", cost_g_t)\n",
        "print(\"---------------------------------------------\")\n",
        "\n",
        "print(\"Using Ideal Learning Rate\")\n",
        "learning_rate_ideal = 0.5\n",
        "\n",
        "print(\"Using X as the feature and Y as the target\")\n",
        "w_i,b_i = gradient_descent_linear_reg_multi_variable(X_ml, Y_ml, gradient_function_multiple_reg, learning_rate_ideal)\n",
        "print(\"Weights:\", w_i)\n",
        "print(\"Bias:\", b_i)\n",
        "print(\"---------------\")\n",
        "cost_i = cost_function_multiple_reg(X, w_i, b_i, Y)\n",
        "print(\"Cost:\", cost_i)\n",
        "print(\"---------------------------------------------\")\n",
        "\n",
        "print(\"Using X_train as the feature and y_train as the target\")\n",
        "w_t_i,b_t_i = gradient_descent_linear_reg_multi_variable(X_train_ml, Y_train_ml, gradient_function_multiple_reg, learning_rate_ideal)\n",
        "print(\"Weights:\", w_t_i)\n",
        "print(\"Bias:\", b_t_i)\n",
        "print(\"---------------\")\n",
        "cost_i_t = cost_function_multiple_reg(X_train, w_t_i, b_t_i, Y_train)\n",
        "print(\"Cost:\", cost_i_t)\n"
      ],
      "metadata": {
        "id": "k1GcsPuGI7nl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# once you have found out the optimal w,b plot the gradient function for [w-100,w+100] and [b-100,b+100]\n",
        "# the shape of the curve is important in the convergence of the gradient descent\n",
        "\n",
        "for i in range(len(w_g)):\n",
        "    plt.plot(X_ml[:, i], Y_ml, 'o', label=f'Data {i+1}')\n",
        "\n",
        "    zx_ml = np.linspace(np.min(X_ml[:, i]) - 0.1, np.max(X_ml[:, i]) + 0.1, 100)\n",
        "    zy_ml = linear_reg_function_multiple(zx_ml, w_g[i], b_g[i])\n",
        "    plt.plot(zx_ml, zy_ml, label=f'Line {i+1}')\n",
        "\n",
        "plt.legend()\n",
        "plt.show()\n",
        "\n",
        "\n",
        "print(\"---------------------------------------------\")\n",
        "\n",
        "for i in range(len(w_t_g)):\n",
        "    plt.plot(X_train_ml[:, i], Y_train_ml, 'o', label=f'Data {i+1}')\n",
        "\n",
        "    zx_t_ml = np.linspace(np.min(X_train_ml[:, i]) - 0.1, np.max(X_train_ml[:, i]) + 0.1, 100)\n",
        "    zy_t_ml = linear_reg_function_multiple(zx_t_ml, w_t_g[i], b_t_g[i])\n",
        "    plt.plot(zx_t_ml, zy_t_ml, label=f'Line {i+1}')\n",
        "\n",
        "plt.legend()\n",
        "plt.show()\n",
        "\n",
        "print(\"-----------------------------------------------------------------------------------\")\n",
        "\n",
        "for i in range(len(w_i)):\n",
        "    plt.plot(X_ml[:, i], Y_ml, 'o', label=f'Data {i+1}')\n",
        "\n",
        "    zx_ml = np.linspace(np.min(X_ml[:, i]) - 0.1, np.max(X_ml[:, i]) + 0.1, 100)\n",
        "    zy_ml = linear_reg_function_multiple(zx_ml, w_i[i], b_i[i])\n",
        "    plt.plot(zx_ml, zy_ml, label=f'Line {i+1}')\n",
        "\n",
        "plt.legend()\n",
        "plt.show()\n",
        "\n",
        "\n",
        "print(\"---------------------------------------------\")\n",
        "\n",
        "\n",
        "for i in range(len(w_t_i)):\n",
        "    plt.plot(X_train_ml[:, i], Y_train_ml, 'o', label=f'Data {i+1}')\n",
        "\n",
        "    zx_t_ml = np.linspace(np.min(X_train_ml[:, i]) - 0.1, np.max(X_train_ml[:, i]) + 0.1, 100)\n",
        "    zy_t_ml = linear_reg_function_multiple(zx_t_ml, w_t_i[i], b_t_i[i])\n",
        "    plt.plot(zx_t_ml, zy_t_ml, label=f'Line {i+1}')\n",
        "\n",
        "plt.legend()\n",
        "plt.show()\n",
        "\n"
      ],
      "metadata": {
        "id": "KuV32sRgJ2yJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Experiment 3 (Optional)\n",
        "Instead of using gradient descent there is another technique which uses the covariance matrix and linear algebra techniques.\n",
        "1. Survey through and find out what that method is.\n",
        "2. Find out the reason why the method is not widely used?"
      ],
      "metadata": {
        "id": "vkBKs6sMB_T6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Ordinart Least Squares (OLS) is the another technique which uses the covariance matrix and linear algebra techniques"
      ],
      "metadata": {
        "id": "M8HYRqIB4S2f"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def o_l_s(X, Y):\n",
        "    X_b = np.hstack((np.ones((X.shape[0], 1)), X))\n",
        "    theta = np.linalg.inv(X_b.T @ X_b) @ X_b.T @ Y\n",
        "    return theta[0], theta[1:]"
      ],
      "metadata": {
        "id": "HOTa8UYcCUl0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def linear_regression_ols(X, Y, o_l_s, learning_rate, bias = 1.0, weight = 1.0, converge_param=0.001):\n",
        "    X_b = np.column_stack((np.ones_like(X), X))\n",
        "    w = weight\n",
        "    b = bias\n",
        "    isConverged = False\n",
        "    num_steps = 1\n",
        "    while (not isConverged):\n",
        "      num_steps += 1\n",
        "      db, dw = o_l_s(X_b,Y)\n",
        "      w -= learning_rate*dw\n",
        "      b -= learning_rate*db\n",
        "      if abs(learning_rate * dw) < converge_param and abs(learning_rate * db) < converge_param:\n",
        "        isConverged = True\n",
        "      weight = w\n",
        "      bias = b\n",
        "    print(\"Converged in \" , num_steps,  \"iterations...\")\n",
        "    return w, b\n"
      ],
      "metadata": {
        "id": "xaZoOvop5cBC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "While Ordinary Least Squares (OLS) is a powerful and intuitive method for linear regression, it has several limitations and drawbacks that make it less widely used in certain scenarios:\n",
        "\n",
        "1. **Computational Complexity**: OLS involves matrix inversion, which has a computational complexity of O(n^3), where n is the number of features. This makes it computationally expensive, especially for large datasets with a high number of features.\n",
        "\n",
        "2. **Sensitivity to Outliers**: OLS is sensitive to outliers in the data. Outliers can heavily influence the estimation of the regression coefficients, leading to biased parameter estimates.\n",
        "\n",
        "3. **Overfitting**: OLS can overfit the training data if the number of features is large compared to the number of observations. This can result in poor generalization performance on unseen data.\n",
        "\n",
        "4. **Multicollinearity**: OLS can produce unstable estimates when the independent variables are highly correlated (multicollinearity). This can lead to inflated standard errors and difficulties in interpreting the coefficients.\n",
        "\n",
        "5. **Non-linearity**: OLS is limited to linear relationships between the independent and dependent variables. If the relationship is non-linear, OLS may not capture the true underlying pattern in the data."
      ],
      "metadata": {
        "id": "xmB52D_o3t2R"
      }
    }
  ]
}
{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Linear regression one variable\n",
        "Dataset : Diabetes dataset in scikit learn"
      ],
      "metadata": {
        "id": "p8n1s49jznjO"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yMJW1BJ9W528"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.datasets import load_diabetes\n",
        "diabetes = load_diabetes(as_frame=True,scaled=True)\n",
        "dataset_dia = diabetes.data\n",
        "\n",
        "print(diabetes.DESCR)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "X = dataset_dia['age'].to_numpy()\n",
        "y = diabetes.target.to_numpy()"
      ],
      "metadata": {
        "id": "FrsJ0BSGlcmI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Linear regression one variable\n",
        "def cost_function(weight,bias,X,y):\n",
        "  J = 0.0\n",
        "  n = len(y)\n",
        "  for i in range(0,n):\n",
        "    J += (weight*X[i]+bias-y[i])**2\n",
        "  J /= (2*n)\n",
        "  return J\n",
        "\n",
        "def get_gradients(weight,bias,X,y):\n",
        "  dJ_dw = 0.0\n",
        "  dJ_db = 0.0\n",
        "  n = len(y)\n",
        "  for i in range(0,n):\n",
        "    dJ_dw += (weight*X[i] + bias -y[i])*X[i]\n",
        "    dJ_db += weight*X[i]+bias-y[i]\n",
        "  dJ_dw /= (n)\n",
        "  dJ_db /= (n)\n",
        "  return dJ_dw, dJ_db\n",
        "\n",
        "def gradient_descent(X, y, weight=1.0, bias=1.0, learning_rate=0.9,threshold=0.1):\n",
        "  isConverged = False\n",
        "  weight_ = weight\n",
        "  bias_ = bias\n",
        "  iter_count = 0\n",
        "  while(not isConverged):\n",
        "    iter_count += 1\n",
        "    dw, db = get_gradients(weight_,bias_,X,y)\n",
        "    weight_ -= learning_rate*dw\n",
        "    bias_ -= learning_rate*db\n",
        "    if(abs(learning_rate*dw)<threshold and abs(learning_rate*db)<threshold):\n",
        "      isConverged = True\n",
        "    # print(weight_, bias_)\n",
        "    weight = weight_\n",
        "    bias = bias_\n",
        "  print(\"Converged in \" , iter_count,  \"iterations...\")\n",
        "  return weight_, bias_"
      ],
      "metadata": {
        "id": "bfw_OeBqYxCp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "w,b = gradient_descent(X,y)"
      ],
      "metadata": {
        "id": "TnwXS-5XdCDe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "plt.plot(X, y, 'o')\n",
        "zx = np.linspace(-0.1, 0.1, 100)\n",
        "zy = w*zx + b\n",
        "plt.plot(zx,zy)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "5hb5PaZxdCZn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Linear regression one variable\n",
        "def cost_function(weight,bias,X,y):\n",
        "  J = 0.0\n",
        "  n = len(y)\n",
        "  for i in range(0,n):\n",
        "    J += (weight*X[i]+bias-y[i])**2\n",
        "  J /= (2*n)\n",
        "  return J\n",
        "\n",
        "def get_gradients(weight,bias,X,y):\n",
        "  dJ_dw = 0.0\n",
        "  dJ_db = 0.0\n",
        "  n = len(y)\n",
        "  for i in range(0,n):\n",
        "    dJ_dw += (weight*X[i] + bias -y[i])*X[i]\n",
        "    dJ_db += weight*X[i]+bias-y[i]\n",
        "  dJ_dw /= (n)\n",
        "  dJ_db /= (n)\n",
        "  return dJ_dw, dJ_db\n",
        "\n",
        "def gradient_descent(X, y, weight=1.0, bias=1.0, learning_rate=0.9,threshold=0.1):\n",
        "  isConverged = False\n",
        "  weight_ = weight\n",
        "  bias_ = bias\n",
        "  iter_count = 0\n",
        "  while(not isConverged):\n",
        "    iter_count += 1\n",
        "    dw, db = get_gradients(weight_,bias_,X,y)\n",
        "    weight_ -= learning_rate*dw\n",
        "    bias_ -= learning_rate*db\n",
        "    if(abs(learning_rate*dw)<threshold and abs(learning_rate*db)<threshold):\n",
        "      isConverged = True\n",
        "    # print(weight_, bias_)\n",
        "    weight = weight_\n",
        "    bias = bias_\n",
        "  print(\"Converged in \" , iter_count,  \"iterations...\")\n",
        "  return weight_, bias_"
      ],
      "metadata": {
        "id": "GSaAP8sKdYXu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Vj6dw_DDfQJ5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(w,b)"
      ],
      "metadata": {
        "id": "wDtHX1lMgIpc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Linear regression one variable with regularization\n",
        "def cost_function_reg(weight,bias,X,y,gamma=0.1):\n",
        "  J = 0.0\n",
        "  n = len(y)\n",
        "  for i in range(0,n):\n",
        "    J += (weight*X[i]+bias-y[i])**2\n",
        "  J /= (2*n) + gamma*(weight**2 + bias**2)\n",
        "  return J\n",
        "\n",
        "def get_gradients_reg(weight,bias,X,y,gamma=0.1):\n",
        "  dJ_dw = 0.0\n",
        "  dJ_db = 0.0\n",
        "  n = len(y)\n",
        "  for i in range(0,n):\n",
        "    dJ_dw += (weight*X[i] + bias -y[i])*X[i]\n",
        "    dJ_db += weight*X[i]+bias-y[i]\n",
        "  dJ_dw /= (n) + 2*gamma*weight\n",
        "  dJ_db /= (n) + 2*gamma*bias\n",
        "  return dJ_dw, dJ_db\n",
        "\n",
        "def gradient_descent_reg(X, y, weight=1.0, bias=1.0, learning_rate=0.9,threshold=0.1):\n",
        "  isConverged = False\n",
        "  weight_ = weight\n",
        "  bias_ = bias\n",
        "  iter_count = 0\n",
        "  while(not isConverged):\n",
        "    iter_count += 1\n",
        "    dw, db = get_gradients_reg(weight_,bias_,X,y)\n",
        "    weight_ -= learning_rate*dw\n",
        "    bias_ -= learning_rate*db\n",
        "    if(abs(learning_rate*dw)<threshold and abs(learning_rate*db)<threshold):\n",
        "      isConverged = True\n",
        "    # print(weight_, bias_)\n",
        "    weight = weight_\n",
        "    bias = bias_\n",
        "  print(\"Converged in \" , iter_count,  \"iterations...\")\n",
        "  return weight_, bias_"
      ],
      "metadata": {
        "id": "P5nL2Xoej1Xs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "w,b = gradient_descent_reg(X,y)"
      ],
      "metadata": {
        "id": "HsuZJix8nrZK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "plt.plot(X, y, 'o')\n",
        "zx = np.linspace(-0.1, 0.1, 100)\n",
        "zy = w*zx + b\n",
        "plt.plot(zx,zy)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "j-g_mO7Sn6-C"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "QJYaE_Fhn9py"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}
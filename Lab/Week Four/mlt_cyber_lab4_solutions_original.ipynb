{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Lab 4 (30-01-2024)\n",
        "\n",
        "This lab experiments help you master how to do linear regression and multiple linear regression.\n",
        "\n",
        "We will be using real estate database provided in lab2.\n",
        "\n"
      ],
      "metadata": {
        "id": "-6H1OBnEnRg5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "Registration_Number = \"22011103020\"\n",
        "Name = \"Jayashre\"\n",
        "\n",
        "# Python Program to Get IP Address\n",
        "import socket\n",
        "hostname = socket.gethostname()\n",
        "IPAddr = socket.gethostbyname(hostname)\n",
        "\n",
        "print(\"My name is \" + Name + \" and my roll no : \" + Registration_Number)\n",
        "print(\"Computer IP Address is: \" + IPAddr)"
      ],
      "metadata": {
        "id": "8YQTqvOLYMMY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "WdvfWDawlu8o"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "dataset = pd.read_csv(\"/content/drive/MyDrive/Colab Notebooks/real_estate.csv\")\n",
        "\n",
        "dataset_two = pd.read_csv(\"/content/drive/MyDrive/Colab Notebooks/HousingData.csv\")\n",
        "\n",
        "re_df = dataset.copy()\n",
        "\n",
        "re_df_two = dataset_two.copy()"
      ],
      "metadata": {
        "id": "M5n83cNIts8-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(re_df.columns)\n",
        "\n",
        "print(\"-------------------\")\n",
        "\n",
        "print(re_df_two.columns)"
      ],
      "metadata": {
        "id": "dsS1ahlkvtm7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "categorical_columns_one = re_df.select_dtypes(include=['object']).columns\n",
        "numerical_columns_one = re_df.select_dtypes(exclude=['object']).columns\n",
        "\n",
        "categorical_columns_two = re_df_two.select_dtypes(include=['object']).columns\n",
        "numerical_columns_two = re_df_two.select_dtypes(exclude=['object']).columns\n",
        "\n",
        "print(categorical_columns_one)\n",
        "print(\"-------------------\")\n",
        "print(numerical_columns_one)\n",
        "print(\"-------------------\")\n",
        "print(categorical_columns_two)\n",
        "print(\"-------------------\")\n",
        "print(numerical_columns_two)"
      ],
      "metadata": {
        "id": "l_HumRS8sACv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Experiment 1 - Predicting House prise using the area of the house using Linear regression\n",
        "Load real estate dataset\n",
        " Split the dataset into\n",
        "\n",
        "1.  Split the dataset into train (90%) and test (10%) using scikit learn\n",
        "2.  Fill the cost function\n",
        "3.  Fill the liner regression fit function\n",
        "4.  Fill the routine for Gradient descent\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "sJLIrF7ooubF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(re_df['Landsize'].isnull().sum())\n",
        "print(\"-------------------\")\n",
        "print(re_df['Price'].isnull().sum())\n",
        "print(\"-------------------\")\n",
        "print(re_df_two['RM'].isnull().sum())\n",
        "print(\"-------------------\")\n",
        "print(re_df_two['MEDV'].isnull().sum())"
      ],
      "metadata": {
        "id": "4T3ll4lBxY4n"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "import numpy as np\n",
        "\n",
        "\n",
        "X_one = re_df[['Landsize']].values\n",
        "Y_one = re_df['Price'].values\n",
        "\n",
        "X_two = re_df_two[['RM']].values\n",
        "Y_two = re_df_two['MEDV'].values\n",
        "\n",
        "# split the dataset into test and train\n",
        "#X_train, X_test, Y_train, Y_test = train_test_split(X_one, Y_one, test_size=0.1, random_state=42)\n",
        "X_train, X_test, Y_train, Y_test = train_test_split(X_two, Y_two, test_size=0.1, random_state=42)"
      ],
      "metadata": {
        "id": "tc4VEc2v1nY0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4MOPlLH4m-Vz"
      },
      "outputs": [],
      "source": [
        "# Experiment 1 - Linear regression with one variable\n",
        "# Fill following functions\n",
        "def linear_reg_function(X,w,b):\n",
        "  \"\"\"\n",
        "  Model function for the\n",
        "  X is the feature vector\n",
        "  Y is the target vector \"For example: \"\n",
        "  b is the bias\n",
        "  w is the weight\n",
        "  \"\"\"\n",
        "  return np.dot(X, w) + b\n",
        "\n",
        "def cost_function(X,w,b,Y):\n",
        "  \"\"\"\n",
        "  Cost function for the linear regression\n",
        "  This is the function that will be minimised using gradient descent\n",
        "  X is the feature vector \"For example: Area of the house\"\n",
        "  Y is the target vector \"For example: Price of the house\"\n",
        "  b is the bias\n",
        "  w is the weight\n",
        "  \"\"\"\n",
        "  m = len(Y)\n",
        "  cf_val = np.sum((linear_reg_function(X, w, b) - Y) ** 2) / (2 * m)\n",
        "  # Compute the cf_val = sum across all i : (wX_i + b)^2\n",
        "\n",
        "  return cf_val\n",
        "\n",
        "def gradient_function(X,w,b,Y):\n",
        "  \"\"\"\n",
        "  Gradient function for the linear regression with one variable\n",
        "  This is the function that will be used to get the gradient in gradient descent\n",
        "  X is the feature vector \"For example: Area of the house\"\n",
        "  Y is the target vector \"For example: Price of the house\"\n",
        "  b is the bias\n",
        "  w is the weight\n",
        "  \"\"\"\n",
        "  m = len(Y)\n",
        "\n",
        "  grad_val_w = np.dot(X.T, (linear_reg_function(X, w, b) - Y)) / m\n",
        "  grad_val_b = np.sum(linear_reg_function(X, w, b) - Y) / m\n",
        "  # Compute the grad_val_w, grad_val_b for the cost function\n",
        "\n",
        "  return grad_val_w, grad_val_b\n",
        "\n",
        "# the below function is a template you may edit it\n",
        "def gradient_descent_linear_reg_one_variable(X,Y,model_func, gradient, learning_rate, converge_param=0.001):\n",
        "  '''\n",
        "  w = np.random.randn(X.shape[1])  # Initialize weights randomly\n",
        "  b = np.random.randn()  # Initialize bias randomly\n",
        "  '''\n",
        "  w = np.random.randn(X.shape[1])\n",
        "  b = np.random.randn()\n",
        "  num_steps = 0\n",
        "  while True:\n",
        "    wt, bt = w, b\n",
        "    cs_val = model_func(X, w, b)\n",
        "    dw, db = gradient(X, w, b, Y)\n",
        "    w -= learning_rate * dw\n",
        "    b -= learning_rate * db\n",
        "    num_steps += 1\n",
        "    if np.abs(w - wt) < converge_param and np.abs(b - bt) < converge_param:\n",
        "      break\n",
        "  return w, b, num_steps\n",
        "\n",
        "learning_rate = 0.01\n",
        "w_opt, b_opt, steps = gradient_descent_linear_reg_one_variable(X_train, Y_train, linear_reg_function, gradient_function, learning_rate)\n",
        "print(\"Optimal weights:\", w_opt)\n",
        "print(\"Optimal bias:\", b_opt)\n",
        "print(\"Number of steps:\", steps)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# once you have found out the optimal w,b plot the gradient function for [w-100,w+100] and [b-100,b+100]\n",
        "# the shape of the curve is important in the convergence of the gradient descent\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "w_values = np.arange(w_opt - 100, w_opt + 100, 1.0)\n",
        "b_values = np.arange(b_opt- 100, b_opt + 100, 1.0)\n",
        "\n",
        "cost_values = np.zeros((len(w_values), len(b_values)))\n",
        "\n",
        "\n",
        "for i, w in enumerate(w_values):\n",
        "    for j, b in enumerate(b_values):\n",
        "        cost_values[i, j] = cost_function(X_two, w, b, Y_two)\n",
        "\n",
        "\n",
        "fig = plt.figure(figsize=(10, 8))\n",
        "ax = fig.add_subplot(111, projection='3d')\n",
        "W, B = np.meshgrid(w_values, b_values)\n",
        "\n",
        "ax.plot_surface(W, B, cost_values.T, cmap='viridis', alpha=0.5)\n",
        "ax.set_xlabel('w')\n",
        "ax.set_ylabel('b')\n",
        "ax.set_zlabel('Cost Function')\n",
        "\n",
        "plt.title('Cost Function Surface')\n",
        "plt.show()\n",
        "\n"
      ],
      "metadata": {
        "id": "QNDTrxNM8lm0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Experminent 2 - Multiple linear regression\n",
        "use more features and modify the code for more than 1 features"
      ],
      "metadata": {
        "id": "TSjBHV8t8c63"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "X_ml = re_df_two[['RM', 'LSTAT']].values\n",
        "Y_ml = re_df_two['MEDV'].values\n",
        "\n",
        "# split the dataset into test and train\n",
        "X_train_ml, X_test_ml, Y_train_ml, Y_test_ml = train_test_split(X_ml, Y_ml, test_size=0.1, random_state=42)\n",
        "\n",
        "sample_size = 500  # Choose the desired size of the training dataset\n",
        "random_indices = np.random.choice(X_train_ml.shape[0], sample_size, replace=True)\n",
        "X_train_ml_sampled = X_train_ml[random_indices]\n",
        "Y_train_ml_sampled = Y_train_ml[random_indices]\n"
      ],
      "metadata": {
        "id": "94hiku8DDxRL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Experiment 2 - Linear regression with tow variables\n",
        "# Fill following functions\n",
        "def multiple_linear_reg_function(X,w,b):\n",
        "  \"\"\"\n",
        "  Model function for the multiple linear regression\n",
        "  X is the feature Matrix \"For example: f features and m samples\"\n",
        "  Y is the target vector \"For example: Price of the house\"\n",
        "  b is the bias\n",
        "  w is the weight vector with length = f-1\n",
        "  \"\"\"\n",
        "\n",
        "  return X.dot(w) + b\n",
        "\n",
        "def cost_function_multiple(X,w,b,Y):\n",
        "  \"\"\"\n",
        "  Cost function for the linear regression\n",
        "  This is the function that will be minimised using gradient descent\n",
        "  X is the feature Matrix \"For example: f features and m samples\"\n",
        "  Y is the target vector \"For example: Price of the house\"\n",
        "  b is the bias\n",
        "  w is the weight vector with length = f-1\n",
        "  \"\"\"\n",
        "  #cf_val = 0.0\n",
        "  # Compute the cf_val = sum across all i : (wX_i + b)^2\n",
        "  m = len(Y)\n",
        "  predictions = multiple_linear_reg_function(X, w, b)\n",
        "  cf_val = (1/(2*m)) * np.sum((predictions - Y)**2)\n",
        "  return cf_val\n",
        "\n",
        "def gradient_function_multiple(X,w,b,Y):\n",
        "  \"\"\"\n",
        "  Gradient function for the linear regression with one variable\n",
        "  This is the function that will be used to get the gradient in gradient descent\n",
        "  X is the feature Matrix \"For example: f features and m samples\"\n",
        "  Y is the target vector \"For example: Price of the house\"\n",
        "  b is the bias\n",
        "  w is the weight vector with length = f-1\n",
        "  \"\"\"\n",
        "  grad_val_w = 0.0\n",
        "  grad_val_b = 0.0\n",
        "  # Compute the grad_val_w, grad_val_b for the cost function\n",
        "  m = len(Y)\n",
        "  predictions = multiple_linear_reg_function(X, w, b)\n",
        "  grad_val_w = (1/m) * X.T.dot(predictions - Y)\n",
        "  grad_val_b = (1/m) * np.sum(predictions - Y)\n",
        "\n",
        "  return grad_val_w, grad_val_b\n",
        "\n",
        "'''\n",
        "def is_converged(W, B, Wt, Bt, converg_param):\n",
        "  if np.abs(W - Wt) < converg_param and np.abs(B - Bt) < converg_param:\n",
        "    return True\n",
        "  return False\n",
        "'''\n",
        "\n",
        "# the below function is a template you may edit it\n",
        "def gradient_descent_linear_reg_multi_variable(X, Y, model_func, gradient, learning_rate, converge_param=0.001):\n",
        "    # Initialise w, b\n",
        "    w = np.zeros(X.shape[1])\n",
        "    b = 0\n",
        "    num_steps = 1\n",
        "    while True:\n",
        "        Wt, Bt = w, b\n",
        "        grad_w, grad_b = gradient(X, w, b, Y)\n",
        "        w = w - learning_rate * grad_w\n",
        "        b = b - learning_rate * grad_b\n",
        "        num_steps += 1\n",
        "        if np.all(np.abs(w - Wt) < converge_param) and np.abs(b - Bt) < converge_param:\n",
        "            break\n",
        "    return w, b, num_steps\n",
        "\n",
        "\n",
        "# Similar to previous case, Test with different learning rates and start values and see how gradient descent works\n",
        "learning_rate = 0.01\n",
        "w_opt, b_opt, steps = gradient_descent_linear_reg_multi_variable(X_train_ml, Y_train_ml, multiple_linear_reg_function, gradient_function_multiple, learning_rate)\n",
        "print(\"Optimal weights:\", w_opt)\n",
        "print(\"Optimal bias:\", b_opt)\n",
        "print(\"Number of steps:\", steps)\n"
      ],
      "metadata": {
        "id": "DG8wpaJb_sFx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Experiment 3 - Muliple regression\n",
        "1. Use the above code with atleast one feature as categorical variable\n",
        "2. Now use different multiple regression for each category of the variable\n",
        "3. Compare the two methods which is better?"
      ],
      "metadata": {
        "id": "OnU4ZOrQAqpA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Use plots to see what is happening with the gradient on case 1 and case 2"
      ],
      "metadata": {
        "id": "1UYFtzBCAFQK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "0w_EPZiwBC6n"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "B96rDLXIB-r8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Experiment 4 (Optional)\n",
        "Instead of using gradient descent there is another technique which uses the covariance matrix and linear algebra techniques.\n",
        "1. Survey through and find out what that method is.\n",
        "2. Find out the reason why the method is not widely used?"
      ],
      "metadata": {
        "id": "vkBKs6sMB_T6"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "HOTa8UYcCUl0"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}